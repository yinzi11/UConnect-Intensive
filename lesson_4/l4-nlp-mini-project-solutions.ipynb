{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Connect Intensive - Machine Learning Nanodegree\n",
    "# Lesson 4: Bayes NLP Mini-Project\n",
    "\n",
    "\n",
    "## Objectives\n",
    "  - Understand how [Bayes Rule](https://en.wikipedia.org/wiki/Bayes%27_theorem) derives from [conditional probability](https://en.wikipedia.org/wiki/Conditional_probability)\n",
    "  - Write methods, utilizing Python dictionary objects and string methods such as `str.split()`.\n",
    "  - Apply Bayes Rule to simple NLP: missing word prediction problems\n",
    "  \n",
    "## Prerequisites\n",
    "  - Basic Python knowledge in strings and dictionaries would help.\n",
    " \n",
    "## Acknowledgements\n",
    "  - This lesson is adapted from one of [Nick Hoh's excellent sessions](https://github.com/nickypie/ConnectIntensive).\n",
    "  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bad Handwriting Exposition\n",
    "\n",
    "\n",
    "Imagine your boss has left you a message from a location with terrible reception. Several words are impossible to hear. Based on some transcriptions of previous messages he’s left, you want to fill in the remaining words. To do this, we will use Bayes’ Rule to find the probability that a given word is in the blank, given some other information about the message.\n",
    "\n",
    "Recall Bayes Rule:\n",
    "\n",
    "P(A|B) = P(B|A)*P(A)/P(B)\n",
    "\n",
    "Or in our case\n",
    "\n",
    "**P(a certain word|surrounding words) = P(surrounding words|a certain word)*P(a certain word) / P(surrounding words)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Calculations\n",
    "\n",
    "Let’s calculate some of the probabilities from the previous page. Here is a message from your boss:\n",
    "\n",
    "“So if you could just go ahead and pack up your stuff and move it down there, that would be terrific, OK?” \n",
    "\n",
    "Assuming the above text is representative, calculate the following probabilities: \n",
    "\n",
    "•\tFinding the word “you” after the word “if”    1    \n",
    "•\tThat a randomly selected word is “you”        .045   \n",
    "•\tThat randomly selected word is “if”               .045     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability 'you': 0.045 \n",
      "Probability 'if': 0.045\n"
     ]
    }
   ],
   "source": [
    "message = 'So if you could just go ahead and pack up your stuff and move it down there, that would be terrific, OK?'\n",
    "message_word_count = message.split()\n",
    "\n",
    "total_word_count = len(message_word_count)\n",
    "count_you = float(0)\n",
    "count_if = float(0)\n",
    "\n",
    "\n",
    "for word in message_word_count:\n",
    "    if word == \"you\":\n",
    "        count_you += 1\n",
    "    if word == \"if\":\n",
    "        count_if += 1 \n",
    "            \n",
    "prob_word_is_you = count_you/total_word_count\n",
    "prob_word_is_if = count_if/total_word_count\n",
    "\n",
    "\n",
    "print (\"Probability 'you': {:.3f} \\nProbability 'if': {:.3f}\".format(prob_word_is_you,prob_word_is_if))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Maximum Likelihood\n",
    "\n",
    "In this exercise, you will write a method, `NextWordProbability(sampletext,word)`, that creates a Python dictionary from a string `sampletext` and a target word `word`. The keys of the dictionary will be the words that follow the target word `word`, and the values will be the number of times the key follows the target word `word`. For example,  the output of the following code:\n",
    "```\n",
    "memo = \"If you could just go ahead and pack up your stuff and move it down there, that would be terrific, OK?\"\n",
    "word = \"and\"\n",
    "print(NextWordProbability(memo,word))\n",
    "```\n",
    "should be the dictionary:\n",
    "```\n",
    "{'move': 1, 'pack': 1}\n",
    "```\n",
    "Don't worry about removing punctuation or changing upper or lower case letters.\n",
    "\n",
    "**Complete** the method `NextWordProbability` in the cell below and then **run** the cell. You may want to use [the string method `split`](https://docs.python.org/2/library/stdtypes.html#string-methods), and refer to [the Python documentation on dictionaries](https://docs.python.org/2/library/stdtypes.html#mapping-types-dict).  Then, you can test your method by running the cell below it to try some test cases. When you feel confident your `NextWordProbability` method works, you can copy and paste the method into the Bayes NLP Mini-Project \"Quiz: Maximum Likelihood\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_memo = '''\n",
    "Milt, we're gonna need to go ahead and move you downstairs into storage B. We have some new people coming in, and we need all the space we can get. So if you could just go ahead and pack up your stuff and move it down there, that would be terrific, OK?\n",
    "Oh, and remember: next Friday... is Hawaiian shirt day. So, you know, if you want to, go ahead and wear a Hawaiian shirt and jeans.\n",
    "Oh, oh, and I almost forgot. Ahh, I'm also gonna need you to go ahead and come in on Sunday, too...\n",
    "Hello Peter, whats happening? Ummm, I'm gonna need you to go ahead and come in tomorrow. So if you could be here around 9 that would be great, mmmk... oh oh! and I almost forgot ahh, I'm also gonna need you to go ahead and come in on Sunday too, kay. We ahh lost some people this week and ah, we sorta need to play catch up.\n",
    "'''\n",
    "\n",
    "#\n",
    "#   Maximum Likelihood Hypothesis\n",
    "#\n",
    "#\n",
    "#   In this quiz we will find the maximum likelihood word based on the preceding word\n",
    "#\n",
    "#   Fill in the NextWordProbability procedure so that it takes in sample text and a word,\n",
    "#   and returns a dictionary with keys the set of words that come after, whose values are\n",
    "#   the number of times the key comes after that word.\n",
    "#   \n",
    "#   Just use .split() to split the sample_memo text into words separated by spaces.\n",
    "\n",
    "def NextWordProbability(sampletext,word):\n",
    "    words = sampletext.split()\n",
    "    prevword = None\n",
    "    d = {}\n",
    "    for w in words:\n",
    "        if prevword == word:\n",
    "            if w in d:\n",
    "                d[w] += 1\n",
    "            else:\n",
    "                d[w] = 1\n",
    "        prevword = w\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'move': 1, 'pack': 1}\n"
     ]
    }
   ],
   "source": [
    "# Test cases: see how well your NextWordProbability method works.\n",
    "\n",
    "memo1 = \"If you could just go ahead and pack up your stuff and move it down there, that would be terrific, OK?\"\n",
    "word1 = \"and\"\n",
    "print(NextWordProbability(memo1,word1))\n",
    "# Output should be:\n",
    "# {'move': 1, 'pack': 1}\n",
    "\n",
    "# memo2 = \"Milt, we're gonna need to go ahead and move you downstairs into storage B. We have some new people coming in, and we need all the space we can get. So if you could just go ahead and pack up your stuff and move it down there, that would be terrific, OK?\"\n",
    "# word2 = \"need\"\n",
    "# print(NextWordProbability(memo2,word2))\n",
    "# # Output should be:\n",
    "# # {'to': 1, 'all': 1}\n",
    "\n",
    "# memo3 = \"Hello Peter, what's happening? Ummm, I'm gonna need you to go ahead and come in tomorrow. So if you could be here around 9 that would be great, mmmk... oh oh! and I almost forgot ahh, I'm also gonna need you to go ahead and come in on Sunday too, kay. We ahh lost some people this week and ah, we sorta need to play catch up.\"\n",
    "# word3 = \"in\"\n",
    "# print(NextWordProbability(memo3,word3))\n",
    "# # Output should be:\n",
    "# # {'tomorrow.': 1, 'on': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Disclaimer\n",
    "\n",
    "In the previous exercise, you may have thought of some ways we might want to clean up the text available to us.\n",
    "\n",
    "For example, we would certainly want to remove punctuation, and generally want to make all strings lowercase for consistency. In most language processing tasks we will have a much larger corpus of data, and will want to remove certain features.\n",
    "\n",
    "Overall, just keep in mind that this mini-project is about Bayesian probability. If you're interested in the details of language processing, you might start with this Kaggle project, which introduces a more detailed and standard approach to text processing very different from what we cover here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimal Classifier Example\n",
    "\n",
    "Imagine that we have two missing words in a row. Rather than simply pick the maximum likelihood possibility for the first blank and using that to estimate the second, we can take the probabilities for all possibilities for the first blank.\n",
    "\n",
    "For example, take the following sentence:\n",
    "\n",
    "“For --- ---“\n",
    "\n",
    "With the following previous data:\n",
    "\n",
    "$$\\begin{array}{rcl}\n",
    "P(\\text{ \"for this\" }|\\text{\"for ---\"})&=&0.4\\\\\n",
    "P(\\text{ \"for that\" }|\\text{\"for ---\"})&=&0.3\\\\\n",
    "P(\\text{ \"for those\" }|\\text{\"for ---\"})&=&0.3\\end{array}$$\n",
    "\n",
    "$$\\begin{array}{rclrcl}\n",
    "P(\\text{ \"this time\" }|\\text{\"this ---\"})&=&0.6,\\quad&P(\\text{ \"this job\" }|\\text{\"this ---\"})&=&0.4\\\\\n",
    "P(\\text{ \"that job\" }|\\text{\"that ---\"})&=&0.8,\\quad&P(\\text{ \"that time\" }|\\text{\"that ---\"})&=&0.2\\\\\n",
    "P(\\text{ \"those items\" }|\\text{\"those ---\"})&=&1.0\\end{array}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**What word should you predict in the second blank?**  job\n",
    "\n",
    "**With what probability?** .4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job:  0.4 \n",
      "time:  0.3 \n",
      "items:  0.3\n"
     ]
    }
   ],
   "source": [
    "p_this= .4\n",
    "p_that = .3 \n",
    "p_those= .3 \n",
    "\n",
    "p_this_time = .6\n",
    "p_this_job = .4\n",
    "\n",
    "p_that_job = .8 \n",
    "p_that_time = .2\n",
    "\n",
    "p_those_items = 1 \n",
    "\n",
    "# getting probability of \"job\"\n",
    "\n",
    "p_job = p_this_job * p_this + p_that_job * p_that\n",
    "\n",
    "# getting probability of \"time\"\n",
    "\n",
    "p_time  = p_this_time * p_this + p_that_time * p_that\n",
    "\n",
    "# getting probability of \"items\"\n",
    "\n",
    "p_items = p_those_items * p_those \n",
    "\n",
    "print \"job: \", p_job,\"\\ntime: \",p_time,\"\\nitems: \", p_items \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Optimal Classifier Exercise\n",
    "\n",
    "In this exercise, you will write a method `LaterWords(sample,word,distance)` that determines the most likely word to appear `distance` words after the target word `word` based on the text in the string `sample`. For example, a call to the method:\n",
    "```\n",
    "LaterWords(memo,\"and\",2)\n",
    "```\n",
    "would return a string: the most frequent word appearing 2 words after `\"and\"` in the string `memo`, *e.g.* \"and --- **---**\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guess: Two blanks after 'ahead' will be: come\n",
      "Guess: Two blanks after 'could' will be: go\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------\n",
    "\n",
    "#\n",
    "#   Bayes Optimal Classifier\n",
    "#\n",
    "#   In this quiz we will compute the optimal label for a second missing word in a row\n",
    "#   based on the possible words that could be in the first blank\n",
    "#\n",
    "#   Finish the procedurce, LaterWords(), below\n",
    "#\n",
    "#   You may want to import your code from the previous programming exercise!\n",
    "#\n",
    "\n",
    "def NextWordProbability(sampletext,word):\n",
    "    words = sampletext.split()\n",
    "    prevword = None\n",
    "    d = {}\n",
    "    for w in words:\n",
    "        if prevword == word:\n",
    "            if w in d:\n",
    "                d[w] += 1\n",
    "            else:\n",
    "                d[w] = 1\n",
    "        prevword = w\n",
    "            \n",
    "    return d\n",
    "\n",
    "\n",
    "sample_memo = '''\n",
    "Milt, we're gonna need to go ahead and move you downstairs into storage B. We have some new people coming in, and we need all the space we can get. So if you could just go ahead and pack up your stuff and move it down there, that would be terrific, OK?\n",
    "Oh, and remember: next Friday... is Hawaiian shirt day. So, you know, if you want to, go ahead and wear a Hawaiian shirt and jeans.\n",
    "Oh, oh, and I almost forgot. Ahh, I'm also gonna need you to go ahead and come in on Sunday, too...\n",
    "Hello Peter, whats happening? Ummm, I'm gonna need you to go ahead and come in tomorrow. So if you could be here around 9 that would be great, mmmk... oh oh! and I almost forgot ahh, I'm also gonna need you to go ahead and come in on Sunday too, kay. We ahh lost some people this week and ah, we sorta need to play catch up.\n",
    "'''\n",
    "\n",
    "corrupted_memo = '''\n",
    "Yeah, I'm gonna --- you to go ahead --- --- complain about this. Oh, and if you could --- --- and sit at the kids' table, that'd be --- \n",
    "'''\n",
    "\n",
    "data_list = sample_memo.strip().split()\n",
    "\n",
    "words_to_guess = [\"ahead\",\"could\"]\n",
    "\n",
    "\n",
    "def LaterWords(sample,word,distance):\n",
    "    '''@param sample: a sample of text to draw from\n",
    "    @param word: a word occuring before a corrupted sequence\n",
    "    @param distance: how many words later to estimate (i.e. 1 for the next word, 2 for the word after that)\n",
    "    @returns: a single word which is the most likely possibility\n",
    "    '''\n",
    "    # Initialize word_dict with the word parameter\n",
    "    word_dict = {word:1}\n",
    "    \n",
    "    for idx in range(distance):\n",
    "             # Create a new dictionary to conduct Naive Bayes\n",
    "        new_dict = {}\n",
    "                # Iterate over each word and word count in word_dict\n",
    "        for w, w_count in word_dict.iteritems():\n",
    "                    # Naive Bayes -- probabilities (hence word counts) are multiplicative:\n",
    "            for new_word, new_count in NextWordProbability(sample,w).iteritems():\n",
    "                        # If the new_word is already in new_dict, add new_count weighted by w_count to the value:\n",
    "                if new_word in new_dict:\n",
    "                        new_dict[new_word] += w_count * new_count\n",
    "                        # If the new_word is not yet in new_dict, initialize with new_count weighted by w_count:\n",
    "                else:\n",
    "                    new_dict[new_word] = w_count * new_count\n",
    "                # Repeat process \"distance\" times, replacing word_dict with new_dict\n",
    "            word_dict = new_dict\n",
    "\n",
    "            # Return the word with the maximum value from Naive Bayes\n",
    "    return max(word_dict.iterkeys(), key=(lambda key: word_dict[key]))\n",
    "    \n",
    "    \n",
    "for i in words_to_guess:\n",
    "    print \"Guess: Two blanks after '{}' will be:\".format(i),LaterWords(sample_memo,i,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Which Words Meditation\n",
    "\n",
    "What set of words in a memo do you think could help predict what a missing word might be? What are some advantages and disadvantages of using more or fewer possible influences in prediction?\n",
    "\n",
    "Many answers\n",
    "\n",
    "Possible: Peoples names, locations and times. The advantage of using more influences is that you could potentially get a more accurate model. The disadvantage is possibly overfitting to the training data. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Joint Distribution Analysis\n",
    "\n",
    "\n",
    "If you wanted to measure the joint probability distribution of a missing word, given its position relative to every other word in the document, how many probabilities would you need to measure? Say the document is $N$ words long.\n",
    "\n",
    "N^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Domain Knowledge Quiz\n",
    "\n",
    "\n",
    "Given the corpus of text we have from our boss, we might like to identify some things he often says, and use that knowledge to make better predictions. What are some statements you see arising multiple times?\n",
    "\n",
    "\"go ahead and\", \"if you could\", \"that would be great\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Domain Knowledge Fill In\n",
    "\n",
    "Trying to search all [regular expressions](https://docs.python.org/2/library/re.html) of length up to 9 with multiple optional parts is computationally infeasible. But if we have these hypotheses to begin with, we can make extremely accurate guesses. For example, fill in the blanks in the following sentence:\n",
    "       > \"Yeah, I'm gonna --- you to go --- --- not complain about this. Oh, and if you could --- ahead and sit at the kids' table, that'd be ---.\"\n",
    "       \n",
    "       \n",
    "       need, ahead, and, go, great\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
