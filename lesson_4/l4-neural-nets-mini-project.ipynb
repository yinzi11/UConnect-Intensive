{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect Intensive - Machine Learning Nanodegree\n",
    "# Lesson 4: Neural Nets Mini-Project\n",
    "\n",
    "\n",
    "## Objectives\n",
    "  - Understand the fundamental concepts underlying [Artificial Neural Networks](https://en.wikipedia.org/wiki/Artificial_neural_network).\n",
    "  - Use Python to develop and test simple Artificial Neural Networks.\n",
    " \n",
    " \n",
    "## Acknowledgements\n",
    "  - This lesson is adapted from one of [Thomas Corcoran's sessions](https://github.com/tccorcoran/Connect).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build a Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ----------\n",
    "# \n",
    "# In this exercise, you will put the finishing touches on a perceptron class.\n",
    "#\n",
    "# Finish writing the activate() method by using np.dot to compute signal\n",
    "# strength and then add in a threshold for perceptron activation.\n",
    "#\n",
    "# ----------\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    This class models an artificial neuron with step activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weights = np.array([1]), threshold = 0):\n",
    "        \"\"\"\n",
    "        Initialize weights and threshold based on input arguments. Note that no\n",
    "        type-checking is being performed here for simplicity.\n",
    "        \"\"\"\n",
    "        self.weights = weights\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def activate(self,inputs):\n",
    "        \"\"\"\n",
    "        Takes in @param inputs, a list of numbers equal to length of weights.\n",
    "        @return the output of a threshold perceptron with given inputs based on\n",
    "        perceptron weights and threshold.\n",
    "        \"\"\" \n",
    "\n",
    "        # INSERT YOUR CODE HERE\n",
    "\n",
    "        # TODO: calculate the strength with which the perceptron fires\n",
    "\n",
    "        # TODO: return 0 or 1 based on the threshold\n",
    "            \n",
    "        return result\n",
    "\n",
    "\n",
    "def test():\n",
    "    \"\"\"\n",
    "    A few tests to make sure that the perceptron class performs as expected.\n",
    "    Nothing should show up in the output if all the assertions pass.\n",
    "    \"\"\"\n",
    "    p1 = Perceptron(np.array([1, 2]), 0.)\n",
    "    assert p1.activate(np.array([ 1,-1])) == 0 # < threshold --> 0\n",
    "    assert p1.activate(np.array([-1, 1])) == 1 # > threshold --> 1\n",
    "    assert p1.activate(np.array([ 2,-1])) == 0 # on threshold --> 0\n",
    "\n",
    "    print \"Tests successfully completed\"\n",
    "if __name__ == \"__main__\":\n",
    "    test()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Threshold Meditation\n",
    "\n",
    "What do you think the advantage of a **perceptron** is, compared with simply returning the dot product without a threshold? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Where to train Perceptrons\n",
    "\n",
    "We want to build networks of **perceptrons** that contain interesting functions. What are the parameters of the **perceptrons** that we will want to modify? \n",
    "\n",
    "•\tOutput functions  \n",
    "•\tInput values  \n",
    "•\tThresholds   \n",
    "•\tWeights   \n",
    "•\tInput functions   \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Perceptron Inputs\n",
    "\n",
    "**Neural networks** are built out of components like perceptron units. What do inputs to networks of perceptrons look like? \n",
    "\n",
    "•\tA matrix of numerical values, with some labeled output.  \n",
    "•\tA directed graph.  \n",
    "•\tAn unlabeled matrix of numerical values.  \n",
    "•\tA set of classifications of numerical values.  \n",
    "•\tA matrix of numerical values with classifications for each row.   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Neural Net Outputs\n",
    "\n",
    "What information can we get as the output of a **neural network**? \n",
    "\n",
    "•\tA directed graph, the neural network itself.   \n",
    "•\tA single scalar-valued number.   \n",
    "•\tThe classification of a vector.  \n",
    "•\tA vector valued output for any vector input.   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Perceptron Update Rule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ----------\n",
    "#\n",
    "# In this exercise, you will update the perceptron class so that it can update\n",
    "# its weights.\n",
    "#\n",
    "# Finish writing the update() method so that it updates the weights according\n",
    "# to the perceptron update rule.\n",
    "# \n",
    "# ----------\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    This class models an artificial neuron with step activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weights = np.array([1]), threshold = 0):\n",
    "        \"\"\"\n",
    "        Initialize weights and threshold based on input arguments. Note that no\n",
    "        type-checking is being performed here for simplicity.\n",
    "        \"\"\"\n",
    "        self.weights = weights\n",
    "        self.threshold = threshold\n",
    "        \n",
    "        print self.weights \n",
    "        print self.threshold\n",
    "\n",
    "\n",
    "    def activate(self, values):\n",
    "        \"\"\"\n",
    "        Takes in @param values, a list of numbers equal to length of weights.\n",
    "        @return the output of a threshold perceptron with given inputs based on\n",
    "        perceptron weights and threshold.\n",
    "        \"\"\"\n",
    "               \n",
    "        # First calculate the strength with which the perceptron fires\n",
    "        strength = np.dot(values,self.weights)\n",
    "        \n",
    "        # Then return 0 or 1 depending on strength compared to threshold  \n",
    "        return int(strength > self.threshold)\n",
    "\n",
    "\n",
    "    def update(self, values, train, eta=.1):\n",
    "        \"\"\"\n",
    "        Takes in a 2D array @param values consisting of a LIST of inputs and a\n",
    "        1D array @param train, consisting of a corresponding list of expected\n",
    "        outputs. Updates internal weights according to the perceptron training\n",
    "        rule using these values and an optional learning rate, @param eta.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "          # YOUR CODE HERE\n",
    "\n",
    "            # TODO: for each data point...\n",
    "        \n",
    "            # TODO: obtain the neuron's prediction for that point\n",
    "\n",
    "            # TODO: update self.weights based on prediction accuracy, learning\n",
    "            # rate and input value\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "def test():\n",
    "    \"\"\"\n",
    "    A few tests to make sure that the perceptron class performs as expected.\n",
    "    Nothing should show up in the output if all the assertions pass.\n",
    "    \"\"\"\n",
    "    def sum_almost_equal(array1, array2, tol = 1e-6):\n",
    "        return sum(abs(array1 - array2)) < tol\n",
    "\n",
    "    p1 = Perceptron(np.array([1,1,1]),0)\n",
    "    p1.update(np.array([[2,0,-3]]), np.array([1]))\n",
    "    assert sum_almost_equal(p1.weights, np.array([1.2, 1, 0.7]))\n",
    "\n",
    "    p2 = Perceptron(np.array([1,2,3]),0)\n",
    "    p2.update(np.array([[3,2,1],[4,0,-1]]),np.array([0,0]))\n",
    "    assert sum_almost_equal(p2.weights, np.array([0.7, 1.8, 2.9]))\n",
    "\n",
    "    p3 = Perceptron(np.array([3,0,2]),0)\n",
    "    p3.update(np.array([[2,-2,4],[-1,-3,2],[0,2,1]]),np.array([0,1,0]))\n",
    "    assert sum_almost_equal(p3.weights, np.array([2.7, -0.3, 1.7]))\n",
    "    \n",
    "print \"Tests successfully completed\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Layered Network Example\n",
    "\n",
    "In general we place units together to form **layered networks**. This will be represented as follows. \n",
    "\n",
    "[[node,node,node], #input layer  \n",
    "[node,node],       #hidden layer  \n",
    "[node]]            #output layer  \n",
    "\n",
    "Given weights for hidden layer of [1,1,-5] and [3,-4,2],  \n",
    "and weights for the output layer of [2,-1],  \n",
    "what will this network output for inputs [1,2,3]? \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "input_layer = [1,2,3]\n",
    "hidden_layer = [1,1,-5]\n",
    "hidden_layer_2 = [3,-4,2]\n",
    "output_layer = [2,-1]\n",
    "\n",
    "step_1 = \n",
    "\n",
    "step_2 = \n",
    "\n",
    "step_3 = \n",
    "\n",
    "step_4 = \n",
    "\n",
    "\n",
    "step_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Linear Representational Power\n",
    "\n",
    "We would like it if these additional layers gave us more representational power. For linear units, just taking weighted sums, they will not. Given the following network, where each node simply passes along the dot product of its inputs with its weights, write down the weights of a single linear node that computes the same function. \n",
    "\n",
    "[[input,input],  \n",
    "[[3,2],[-1,4],[3,-5]],  \n",
    "[[1,2,-1]]]  \n",
    "\n",
    "\n",
    "[,]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Build the XOR Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ----------\n",
    "#\n",
    "# In this exercise, you will create a network of perceptrons that can represent\n",
    "# the XOR function, using a network structure like those shown in the previous\n",
    "# quizzes.\n",
    "#\n",
    "# You will need to do two things:\n",
    "# First, create a network of perceptrons with the correct weights\n",
    "# Second, define a procedure EvalNetwork() which takes in a list of inputs and\n",
    "# outputs the value of this network.\n",
    "#\n",
    "# ----------\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    This class models an artificial neuron with step activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weights = np.array([1]), threshold = 0):\n",
    "        \"\"\"\n",
    "        Initialize weights and threshold based on input arguments. Note that no\n",
    "        type-checking is being performed here for simplicity.\n",
    "        \"\"\"\n",
    "        self.weights = weights\n",
    "        self.threshold = threshold\n",
    "\n",
    "\n",
    "    def activate(self, values):\n",
    "        \"\"\"\n",
    "        Takes in @param values, a list of numbers equal to length of weights.\n",
    "        @return the output of a threshold perceptron with given inputs based on\n",
    "        perceptron weights and threshold.\n",
    "        \"\"\"\n",
    "               \n",
    "        # First calculate the strength with which the perceptron fires\n",
    "        strength = np.dot(values,self.weights)\n",
    "        \n",
    "        # Then return 0 or 1 depending on strength compared to threshold  \n",
    "        return int(strength > self.threshold)\n",
    "    \n",
    "\n",
    "# Part 1: Set up the perceptron network\n",
    "Network = [\n",
    "    # input layer, declare input layer perceptrons here\n",
    "    [ ... ], \\\n",
    "    # output node, declare output layer perceptron here\n",
    "    [ ... ]\n",
    "]\n",
    "\n",
    "# Part 2: Define a procedure to compute the output of the network, given inputs\n",
    "def EvalNetwork(inputValues, Network):\n",
    "    \"\"\"\n",
    "    Takes in @param inputValues, a list of input values, and @param Network\n",
    "    that specifies a perceptron network. @return the output of the Network for\n",
    "    the given set of inputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Be sure your output value is a single number\n",
    "    return OutputValue\n",
    "\n",
    "def test():\n",
    "    \"\"\"\n",
    "    A few tests to make sure that the perceptron class performs as expected.\n",
    "    \"\"\"\n",
    "    print \"0 XOR 0 = 0?:\", EvalNetwork(np.array([0,0]), Network)\n",
    "    print \"0 XOR 1 = 1?:\", EvalNetwork(np.array([0,1]), Network)\n",
    "    print \"1 XOR 0 = 1?:\", EvalNetwork(np.array([1,0]), Network)\n",
    "    print \"1 XOR 1 = 0?:\", EvalNetwork(np.array([1,1]), Network)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Discretion Quiz\n",
    "\n",
    "One problem that perceptron units have is that their outputs are discrete. This makes it difficult to address regression problems with them, and requires them to have more complexity to capture some concepts. \n",
    "\n",
    "For example: \n",
    "\n",
    "Given a network of perceptrons with structure [2,2,1], that is, two input nodes, two hidden nodes, and one output node, how many possible prices could the network assign to a house? \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Activation Function Sandbox\n",
    "\n",
    "As discussed in the previous lesson, to solve the problem of having only a very few discrete outputs from our neural net, we'll apply a transition function.\n",
    "\n",
    "We'll start by letting you test out a variety of functions, numerically approximating their derivatives in order to apply a gradient descent update rule.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This will have to be conducted within the online quiz \n",
    "\n",
    "\n",
    "# ----------\n",
    "# \n",
    "# Python Neural Networks code originally by Szabo Roland and used with\n",
    "# permission\n",
    "#\n",
    "# Modifications, comments, and exercise breakdowns by Mitchell Owen,\n",
    "# (c) Udacity\n",
    "#\n",
    "# Retrieved originally from http://rolisz.ro/2013/04/18/neural-networks-in-python/\n",
    "#\n",
    "#\n",
    "# Neural Network Sandbox\n",
    "#\n",
    "# Define an activation function activate(), which takes in a number and\n",
    "# returns a number.\n",
    "# Using test run you can see the performance of a neural network running with\n",
    "# that activation function, where the inputs are 8x8 images of digits (0-9) and\n",
    "# the outputs are digit predictions made by the network.\n",
    "#\n",
    "# ----------\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def activate(strength):\n",
    "    # Try out different functions here. Input strength will be a number, with\n",
    "    # another number as output.\n",
    "    return np.power(strength,2)\n",
    "    \n",
    "def activation_derivative(activate, strength):\n",
    "    #numerically approximate\n",
    "    return (activate(strength+1e-5)-activate(strength-1e-5))/(2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Activation Function Quiz\n",
    "\n",
    "We have decided that we need a function which is continuous (to avoid the discrete problem of perceptrons) but not linear (to allow us to represent non-linear functions). Which of the following seems appropriate? \n",
    "\n",
    "\n",
    "•\tSine  \n",
    "•\tArctangent   \n",
    "•\tNatural logarithm   \n",
    "•\tCube root  \n",
    "•\tLogistic function   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Perceptron Vs Sigmoid\n",
    "\n",
    "\n",
    "What will the difference be between a single perceptron and a sigmoid unit on binary classification problems? \n",
    "\n",
    "•\tThere will be no difference.   \n",
    "•\tOne gives more information but they will give the same answer.   \n",
    "•\tThey will sometimes give different answers.   \n",
    "•\tThey will always give different answers.  \n",
    "•\tThey will give different answers in certain rare circumstances.   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Sigmoid Learning\n",
    "\n",
    "We need to train our networks of sigmoid units like we trained networks of perceptrons. How should we determine our update rules? \n",
    "\n",
    "•\tArbitrarily.   \n",
    "•\tUsing our intuition.    \n",
    "•\tUsing domain knowledge.   \n",
    "•\tUsing calculus.   \n",
    "•\tUsing trigonometry.   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Gradient Descent Issues\n",
    "\n",
    "Using calculus, **gradient descent** can provide us with a locally optimal set of weight changes, under certain assumptions. However, some issues can arise. Which of the following do you think could be problematic? \n",
    "\n",
    "•\tLocal extrema.   \n",
    "•\tLengthy run times.   \n",
    "•\tInfinite loops.  \n",
    "•\tFailure to completely converge.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Sigmoid Programming Exercise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ----------\n",
    "# \n",
    "# As with the previous perceptron exercises, you will complete some of the core\n",
    "# methods of a sigmoid unit class.\n",
    "#\n",
    "# There are two functions for you to finish:\n",
    "# First, in activate(), write the sigmoid activation function.\n",
    "# Second, in update(), write the gradient descent update rule. Updates should be\n",
    "#   performed online, revising the weights after each data point.\n",
    "# \n",
    "# ----------\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    \"\"\"\n",
    "    This class models an artificial neuron with sigmoid activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weights = np.array([1])):\n",
    "        \"\"\"\n",
    "        Initialize weights based on input arguments. Note that no type-checking\n",
    "        is being performed here for simplicity of code.\n",
    "        \"\"\"\n",
    "        self.weights = weights\n",
    "\n",
    "        # NOTE: You do not need to worry about these two attribues for this\n",
    "        # programming quiz, but these will be useful for if you want to create\n",
    "        # a network out of these sigmoid units!\n",
    "        self.last_input = 0 # strength of last input\n",
    "        self.delta      = 0 # error signal\n",
    "    \n",
    "    def activate(self, values):\n",
    "        \"\"\"\n",
    "        Takes in @param values, a list of numbers equal to length of weights.\n",
    "        @return the output of a sigmoid unit with given inputs based on unit\n",
    "        weights.\n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # First calculate the strength of the input signal.\n",
    "        strength = np.dot(values, self.weights)\n",
    "        self.last_input = strength\n",
    "        \n",
    "        # TODO: Modify strength using the sigmoid activation function and\n",
    "        # return as output signal.\n",
    "        # HINT: You may want to create a helper function to compute the\n",
    "        #   logistic function since you will need it for the update function.\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def update(self, values, train, eta=.1):\n",
    "        \"\"\"\n",
    "        Takes in a 2D array @param values consisting of a LIST of inputs and a\n",
    "        1D array @param train, consisting of a corresponding list of expected\n",
    "        outputs. Updates internal weights according to gradient descent using\n",
    "        these values and an optional learning rate, @param eta.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: for each data point...\n",
    "        for X, y_true in zip(values, train):\n",
    "            # obtain the output signal for that point\n",
    "            y_pred = self.activate(X)\n",
    "\n",
    "            # YOUR CODE HERE\n",
    "\n",
    "            # TODO: compute derivative of logistic function at input strength\n",
    "            # Recall: d/dx logistic(x) = logistic(x)*(1-logistic(x))\n",
    "\n",
    "            # TODO: update self.weights based on learning rate, signal accuracy,\n",
    "            # function slope (derivative) and input value\n",
    "            \n",
    "\n",
    "def test():\n",
    "    \"\"\"\n",
    "    A few tests to make sure that the perceptron class performs as expected.\n",
    "    Nothing should show up in the output if all the assertions pass.\n",
    "    \"\"\"\n",
    "    def sum_almost_equal(array1, array2, tol = 1e-5):\n",
    "        return sum(abs(array1 - array2)) < tol\n",
    "\n",
    "    u1 = Sigmoid(weights=[3,-2,1])\n",
    "    assert abs(u1.activate(np.array([1,2,3])) - 0.880797) < 1e-5\n",
    "    \n",
    "    u1.update(np.array([[1,2,3]]),np.array([0]))\n",
    "    assert sum_almost_equal(u1.weights, np.array([2.990752, -2.018496, 0.972257]))\n",
    "\n",
    "    u2 = Sigmoid(weights=[0,3,-1])\n",
    "    u2.update(np.array([[-3,-1,2],[2,1,2]]),np.array([1,0]))\n",
    "    assert sum_almost_equal(u2.weights, np.array([-0.030739, 2.984961, -1.027437]))\n",
    "    return True\n",
    "if  test():\n",
    "    print \"All tests completed sucessfully\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
