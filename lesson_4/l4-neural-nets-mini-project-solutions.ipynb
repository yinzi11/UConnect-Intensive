{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect Intensive - Machine Learning Nanodegree\n",
    "# Lesson 4: Neural Nets Mini-Project\n",
    "\n",
    "\n",
    "## Objectives\n",
    "  - Understand the fundamental concepts underlying [Artificial Neural Networks](https://en.wikipedia.org/wiki/Artificial_neural_network).\n",
    "  - Use Python to develop and test simple Artificial Neural Networks.\n",
    " \n",
    " \n",
    "## Acknowledgements\n",
    "  - This lesson is adapted from one of [Thomas Corcoran's sessions](https://github.com/tccorcoran/Connect).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build a Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ----------\n",
    "# \n",
    "# In this exercise, you will put the finishing touches on a perceptron class.\n",
    "#\n",
    "# Finish writing the activate() method by using np.dot to compute signal\n",
    "# strength and then add in a threshold for perceptron activation.\n",
    "#\n",
    "# ----------\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    This class models an artificial neuron with step activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weights = np.array([1]), threshold = 0):\n",
    "        \"\"\"\n",
    "        Initialize weights and threshold based on input arguments. Note that no\n",
    "        type-checking is being performed here for simplicity.\n",
    "        \"\"\"\n",
    "        self.weights = weights\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def activate(self,inputs):\n",
    "        \"\"\"\n",
    "        Takes in @param inputs, a list of numbers equal to length of weights.\n",
    "        @return the output of a threshold perceptron with given inputs based on\n",
    "        perceptron weights and threshold.\n",
    "        \"\"\" \n",
    "\n",
    "        # INSERT YOUR CODE HERE\n",
    "\n",
    "        # TODO: calculate the strength with which the perceptron fires\n",
    "        \n",
    "        strength = np.dot(self.weights,inputs)\n",
    "        \n",
    "        \n",
    "        # TODO: return 0 or 1 based on the threshold\n",
    "        \n",
    "        if strength <= self.threshold:\n",
    "            result = 0\n",
    "        else:\n",
    "            result = 1\n",
    "\n",
    "      \n",
    "        print \"strength\", strength, \"result: \", result, \"\\n\"\n",
    "                                                                        \n",
    "      \n",
    "       \n",
    "        return result\n",
    "\n",
    "\n",
    "def test():\n",
    "    \"\"\"\n",
    "    A few tests to make sure that the perceptron class performs as expected.\n",
    "    \"\"\"\n",
    "    p1 = Perceptron(np.array([1, 2]), 0.)# assigned weights of 1 & 2 and threshold 0 \n",
    "    assert p1.activate(np.array([ 1,-1])) == 0 # strength = 1*1 + 2*-1 = -1: -1 < threshold so return 0 \n",
    "    assert p1.activate(np.array([-1, 1])) == 1 # strength = 1*-1 + 2*1 = 1: 1 > threshold so return 1\n",
    "    assert p1.activate(np.array([ 2,-1])) == 0 # strength = 1*2 + 2*-1 = 0: 0 = threshold so return 0\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    test()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Threshold Meditation\n",
    "\n",
    "What do you think the advantage of a **perceptron** is, compared with simply returning the dot product without a threshold? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Where to train Perceptrons\n",
    "\n",
    "We want to build networks of **perceptrons** that contain interesting functions. What are the parameters of the **perceptrons** that we will want to modify? \n",
    "\n",
    "•\tOutput functions  \n",
    "•\tInput values  \n",
    "•\tThresholds X  \n",
    "•\tWeights X  \n",
    "•\tInput functions   \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Perceptron Inputs\n",
    "\n",
    "**Neural networks** are built out of components like perceptron units. What do inputs to networks of perceptrons look like? \n",
    "\n",
    "•\tA matrix of numerical values, with some labeled output.  \n",
    "•\tA directed graph.  \n",
    "•\tAn unlabeled matrix of numerical values.  \n",
    "•\tA set of classifications of numerical values.  \n",
    "•\tA matrix of numerical values with classifications for each row. X  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Neural Net Outputs\n",
    "\n",
    "What information can we get as the output of a **neural network**? \n",
    "\n",
    "•\tA directed graph, the neural network itself. X  \n",
    "•\tA single scalar-valued number. X  \n",
    "•\tThe classification of a vector. X  \n",
    "•\tA vector valued output for any vector input. X  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Perceptron Update Rule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ----------\n",
    "#\n",
    "# In this exercise, you will update the perceptron class so that it can update\n",
    "# its weights.\n",
    "#\n",
    "# Finish writing the update() method so that it updates the weights according\n",
    "# to the perceptron update rule.\n",
    "# \n",
    "# ----------\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    This class models an artificial neuron with step activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weights = np.array([1]), threshold = 0):\n",
    "        \"\"\"\n",
    "        Initialize weights and threshold based on input arguments. Note that no\n",
    "        type-checking is being performed here for simplicity.\n",
    "        \"\"\"\n",
    "        self.weights = weights\n",
    "        self.threshold = threshold\n",
    "\n",
    "\n",
    "    def activate(self, values):\n",
    "        \"\"\"\n",
    "        Takes in @param values, a list of numbers equal to length of weights.\n",
    "        @return the output of a threshold perceptron with given inputs based on\n",
    "        perceptron weights and threshold.\n",
    "        \"\"\"\n",
    "               \n",
    "        # First calculate the strength with which the perceptron fires\n",
    "        strength = np.dot(values,self.weights)\n",
    "        \n",
    "        print \"from activate():\"\n",
    "        print \"values: \",values\n",
    "        print \"weights: \", self.weights\n",
    "        print \"strength: \", strength, \"\\n\"\n",
    "        \n",
    "        # Then return 0 or 1 depending on strength compared to threshold  \n",
    "        return int(strength > self.threshold)\n",
    "\n",
    "\n",
    "    def update(self, values, expected_output, eta=.1):\n",
    "        \"\"\"\n",
    "        Takes in a 2D array @param values consisting of a LIST of inputs and a\n",
    "        1D array @param train, consisting of a corresponding list of expected\n",
    "        outputs. Updates internal weights according to the perceptron training\n",
    "        rule using these values and an optional learning rate, @param eta.\n",
    "        \"\"\"\n",
    "     \n",
    "        for (index, inputs) in enumerate(values):\n",
    "            predicted_output = self.activate(inputs)\n",
    "            previous_weights = self.weights\n",
    "            self.weights = (eta * (expected_output[index] - predicted_output)) * inputs + self.weights  \n",
    "\n",
    "            print \"from update():\"\n",
    "            print \"index (item in list): \", index\n",
    "            print \"inputs: \", inputs\n",
    "            print \"expected_output: \", expected_output[index]\n",
    "            print \"predicted_output:\", predicted_output\n",
    "            print \"expected_output - predicted_output: \",(expected_output[index] - predicted_output)\n",
    "            print \"weight update formula: \", \"(eta * (expected_output[index] - predicted_output)) * inputs + self.weights\"\n",
    "            print \"weight update formula-values: \", eta, \"* (\",expected_output[index], \"-\", predicted_output,\") *\", inputs, \"+\", previous_weights   \n",
    "            print \"updated weights: \", self.weights\n",
    "            print \"\\n\"\n",
    "\n",
    "\n",
    "def test():\n",
    "    \"\"\"\n",
    "    A few tests to make sure that the perceptron class performs as expected.\n",
    "    \"\"\"\n",
    "    def sum_almost_equal(array1, array2, tol = 1e-6):# a function that determines if updates calculated are same as expected \n",
    "        return sum(abs(array1 - array2)) < tol\n",
    "\n",
    "    p1 = Perceptron(np.array([1,1,1]),0) # Setting the perceptron's initial weights and threshold\n",
    "    p1.update(np.array([[2,0,-3]]), np.array([1]))# Updating the weights by giving inputs and expected outputs\n",
    "    assert sum_almost_equal(p1.weights, np.array([1.2, 1, 0.7]))#After updating the weights the new weights should be [1.2,1,0.7] \n",
    "\n",
    "#     p2 = Perceptron(np.array([1,2,3]),0)\n",
    "#     p2.update(np.array([[3,2,1],[4,0,-1]]),np.array([0,0]))\n",
    "#     assert sum_almost_equal(p2.weights, np.array([0.7, 1.8, 2.9]))\n",
    "\n",
    "#     p3 = Perceptron(np.array([3,0,2]),0)\n",
    "#     p3.update(np.array([[2,-2,4],[-1,-3,2],[0,2,1]]),np.array([0,1,0]))\n",
    "#     assert sum_almost_equal(p3.weights, np.array([2.7, -0.3, 1.7]))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Layered Network Example\n",
    "\n",
    "In general we place units together to form **layered networks**. This will be represented as follows. \n",
    "\n",
    "[[node,node,node], #input layer  \n",
    "[node,node],       #hidden layer  \n",
    "[node]]            #output layer  \n",
    "\n",
    "Given weights for hidden layer of [1,1,-5] and [3,-4,2],  \n",
    "and weights for the output layer of [2,-1],  \n",
    "what will this network output for inputs [1,2,3]? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "input_layer = [1,2,3]\n",
    "hidden_layer_weights = [1,1,-5]\n",
    "hidden_layer_weights_2 = [3,-4,2]\n",
    "output_layer = [2,-1]\n",
    "\n",
    "step_1 = np.dot(input_layer,hidden_layer_weights)#calculate the dot product of inputs and first hidden layer weights \n",
    "\n",
    "step_2 = np.dot(input_layer,hidden_layer_weights_2)#calculate the dot product of inputs and second hidden layer weights \n",
    "\n",
    "step_3 = [step_1,step_2] #combine outputs \n",
    "\n",
    "step_4 = np.dot(step_3,output_layer) #calculate the dot product of output layer weights and hidden layer outputs \n",
    "\n",
    "\n",
    "step_4 # display the output for the network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Linear Representational Power\n",
    "\n",
    "We would like it if these additional layers gave us more representational power. For linear units, just taking weighted sums, they will not. Given the following network, where each node simply passes along the dot product of its inputs with its weights, write down the weights of a single linear node that computes the same function. \n",
    "\n",
    "[[input,input],  \n",
    "[[3,2],[-1,4],[3,-5]],  \n",
    "[[1,2,-1]]]  \n",
    "\n",
    "First layer\n",
    "[3a + 2b,\n",
    " -a + 4b,\n",
    " 3a -5b]\n",
    " \n",
    "Output layer\n",
    "[3a + 2b,\n",
    "-2a + 8b,\n",
    "-3a + 5b]\n",
    " \n",
    "-2a + 15b \n",
    " \n",
    " \n",
    "\n",
    "[-2,15]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Build the XOR Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ----------\n",
    "#\n",
    "# In this exercise, you will create a network of perceptrons that can represent\n",
    "# the XOR function, using a network structure like those shown in the previous\n",
    "# quizzes.\n",
    "#\n",
    "# You will need to do two things:\n",
    "# First, create a network of perceptrons with the correct weights\n",
    "# Second, define a procedure EvalNetwork() which takes in a list of inputs and\n",
    "# outputs the value of this network.\n",
    "#\n",
    "# ----------\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "   \n",
    "    def __init__(self, weights = np.array([1]), threshold = 0):\n",
    "      \n",
    "        self.weights = weights\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def activate(self, values):\n",
    "     \n",
    "        strength = np.dot(values,self.weights)\n",
    "        \n",
    "        return int(strength > self.threshold)\n",
    "\n",
    "Network = [\n",
    "\n",
    "    [ Perceptron([1, 1], 1.5 )], \\\n",
    " \n",
    "    [ Perceptron([1, 1, -2], .5 )]\n",
    "]\n",
    "\n",
    "# There are an infinite number of weights which can be used for the Network\n",
    "\n",
    "def EvalNetwork(inputValues, Network):\n",
    "    \n",
    "        values = inputValues.tolist()\n",
    "        values.append(Network[0][0].activate(values))\n",
    "        #Runs the 1st Network weights/threshold through the activate function with \"values\" and appends the output to \"values\"\n",
    "        #Initial input values for the first print statement: [0,0]. strength = 0*1 + 0*1 = 0:  0 < 1.5 threshold so output 0\n",
    "        #append the output to \"values\": [0,0,0]\n",
    "\n",
    "        \n",
    "        OutputValue = Network[1][0].activate(values)\n",
    "        #Runs the new input values (with the appended value) through the activate function with the 2nd Network weights/threshold  \n",
    "        #New input values for first print statement [0,0,0] 2nd Network list [1,1,-2] threshold .5 \n",
    "        # strength = 0*1 + 0*1 + 0*-2 = 0: 0 < .5 threshold so output 0\n",
    " \n",
    "        \n",
    "        return OutputValue\n",
    "\n",
    "\n",
    "def test():\n",
    "    \"\"\"\n",
    "    A few tests to make sure that the perceptron class performs as expected.\n",
    "    \"\"\"\n",
    "    print \"0 XOR 0 = 0?:\", EvalNetwork(np.array([0,0]), Network)\n",
    "    print \"0 XOR 1 = 1?:\", EvalNetwork(np.array([0,1]), Network)\n",
    "    print \"1 XOR 0 = 1?:\", EvalNetwork(np.array([1,0]), Network)\n",
    "    print \"1 XOR 1 = 0?:\", EvalNetwork(np.array([1,1]), Network)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Discretion Quiz\n",
    "\n",
    "One problem that perceptron units have is that their outputs are discrete. This makes it difficult to address regression problems with them, and requires them to have more complexity to capture some concepts. \n",
    "\n",
    "For example: \n",
    "\n",
    "Given a network of perceptrons with structure [2,2,1], that is, two input nodes, two hidden nodes, and one output node, how many possible prices could the network assign to a house? \n",
    "\n",
    "4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Activation Function Sandbox\n",
    "\n",
    "As discussed in the previous lesson, to solve the problem of having only a very few discrete outputs from our neural net, we'll apply a transition function.\n",
    "\n",
    "We'll start by letting you test out a variety of functions, numerically approximating their derivatives in order to apply a gradient descent update rule.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This will have to be conducted within the online quiz \n",
    "\n",
    "\n",
    "# ----------\n",
    "# \n",
    "# Python Neural Networks code originally by Szabo Roland and used with\n",
    "# permission\n",
    "#\n",
    "# Modifications, comments, and exercise breakdowns by Mitchell Owen,\n",
    "# (c) Udacity\n",
    "#\n",
    "# Retrieved originally from http://rolisz.ro/2013/04/18/neural-networks-in-python/\n",
    "#\n",
    "#\n",
    "# Neural Network Sandbox\n",
    "#\n",
    "# Define an activation function activate(), which takes in a number and\n",
    "# returns a number.\n",
    "# Using test run you can see the performance of a neural network running with\n",
    "# that activation function, where the inputs are 8x8 images of digits (0-9) and\n",
    "# the outputs are digit predictions made by the network.\n",
    "#\n",
    "# ----------\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def activate(strength):\n",
    "    # Try out different functions here. Input strength will be a number, with\n",
    "    # another number as output.\n",
    "    return np.power(strength,2)\n",
    "    \n",
    "def activation_derivative(activate, strength):\n",
    "    #numerically approximate\n",
    "    return (activate(strength+1e-5)-activate(strength-1e-5))/(2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Activation Function Quiz\n",
    "\n",
    "We have decided that we need a function which is continuous (to avoid the discrete problem of perceptrons) but not linear (to allow us to represent non-linear functions). Which of the following seems appropriate? \n",
    "\n",
    "\n",
    "•\tSine  \n",
    "•\tArctangent   \n",
    "•\tNatural logarithm   \n",
    "•\tCube root  \n",
    "•\tLogistic function X  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Perceptron Vs Sigmoid\n",
    "\n",
    "\n",
    "What will the difference be between a single perceptron and a sigmoid unit on binary classification problems? \n",
    "\n",
    "•\tThere will be no difference.   \n",
    "•\tOne gives more information but they will give the same answer.X   \n",
    "•\tThey will sometimes give different answers.   \n",
    "•\tThey will always give different answers.  \n",
    "•\tThey will give different answers in certain rare circumstances.   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Sigmoid Learning\n",
    "\n",
    "We need to train our networks of sigmoid units like we trained networks of perceptrons. How should we determine our update rules? \n",
    "\n",
    "•\tArbitrarily.   \n",
    "•\tUsing our intuition.    \n",
    "•\tUsing domain knowledge.   \n",
    "•\tUsing calculus. X  \n",
    "•\tUsing trigonometry.   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Gradient Descent Issues\n",
    "\n",
    "Using calculus, **gradient descent** can provide us with a locally optimal set of weight changes, under certain assumptions. However, some issues can arise. Which of the following do you think could be problematic? \n",
    "\n",
    "•\tLocal extrema. X  \n",
    "•\tLengthy run times. X  \n",
    "•\tInfinite loops.X  \n",
    "•\tFailure to completely converge. X  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Sigmoid Programming Exercise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ----------\n",
    "# \n",
    "# As with the previous perceptron exercises, you will complete some of the core\n",
    "# methods of a sigmoid unit class.\n",
    "#\n",
    "# There are two functions for you to finish:\n",
    "# First, in activate(), write the sigmoid activation function.\n",
    "# Second, in update(), write the gradient descent update rule. Updates should be\n",
    "#   performed online, revising the weights after each data point.\n",
    "# \n",
    "# ----------\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    \"\"\"\n",
    "    This class models an artificial neuron with sigmoid activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weights = np.array([1])):\n",
    "        \"\"\"\n",
    "        Initialize weights based on input arguments. Note that no type-checking\n",
    "        is being performed here for simplicity of code.\n",
    "        \"\"\"\n",
    "        self.weights = weights\n",
    "\n",
    "        # NOTE: You do not need to worry about these two attribues for this\n",
    "        # programming quiz, but these will be useful for if you want to create\n",
    "        # a network out of these sigmoid units!\n",
    "        self.last_input = 0 # strength of last input\n",
    "        self.delta      = 0 # error signal\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def logistic(self,x):\n",
    "        return 1/ (1 + np.exp(-x))# creating a logistic function which will be used to update the weights\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    def activate(self, values):\n",
    "        \"\"\"\n",
    "        Takes in @param values, a list of numbers equal to length of weights.\n",
    "        @return the output of a sigmoid unit with given inputs based on unit\n",
    "        weights.\n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # First calculate the strength of the input signal.\n",
    "        strength = np.dot(values, self.weights)\n",
    "        self.last_input = strength\n",
    "        \n",
    "      \n",
    "    \n",
    "        \n",
    "        # TODO: Modify strength using the sigmoid activation function and\n",
    "        # return as output signal.\n",
    "        # HINT: You may want to create a helper function to compute the\n",
    "        #   logistic function since you will need it for the update function.\n",
    "        \n",
    "        return self.logistic(strength)# inputting the strength into the logistic function to get y_pred\n",
    "    \n",
    "    def update(self, values, expected_output, eta=.1):\n",
    "        \"\"\"\n",
    "        Takes in a 2D array @param values consisting of a LIST of inputs and a\n",
    "        1D array @param train, consisting of a corresponding list of expected\n",
    "        outputs. Updates internal weights according to gradient descent using\n",
    "        these values and an optional learning rate, @param eta.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: for each data point...\n",
    "        for X, y_true in zip(values, expected_output):\n",
    "            # obtain the output signal for that point\n",
    "            y_pred = self.activate(X)\n",
    "            \n",
    "            initial_weights = self.weights #will be used in some of the formula visualizations\n",
    "            \n",
    "            \n",
    "            \n",
    "        # TODO: update self.weights based on learning rate, signal accuracy,\n",
    "            # function slope (derivative) and input value\n",
    "            # YOUR CODE HERE\n",
    "            \n",
    "            \n",
    "            ddx = y_pred * (1 - y_pred)# calculating the derivative of y_pred\n",
    "            delta_w = eta *(y_true - y_pred) * ddx * X #calculating the change in the weights \n",
    "            self.weights = self.weights + delta_w # updating the weights by adding the change to the initial weights\n",
    " \n",
    "            \n",
    "            \n",
    "            \n",
    "            print \"values: \", values \n",
    "            print \"initial_weights: \", initial_weights \n",
    "            print \"y_pred: \", y_pred \n",
    "            print \"y_true: \", y_true\n",
    "            print \"weight change formula: eta *(y_true - y_pred) * ddx * X\"\n",
    "            print \"formula values:\", eta, \"* (\",y_true, \"-\", y_pred,\") *\", ddx,\"*\", X \n",
    "            print \"delta_w: \",delta_w\n",
    "            print \"updated weights formula: \", initial_weights, \"+\", delta_w\n",
    "            print \"updated weights: \", self.weights, \"\\n\"\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "def test():\n",
    "    \"\"\"\n",
    "    A few tests to make sure that the perceptron class performs as expected.\n",
    "    \"\"\"\n",
    "    def sum_almost_equal(array1, array2, tol = 1e-5):\n",
    "        return sum(abs(array1 - array2)) < tol\n",
    "\n",
    "    u1 = Sigmoid(weights=[3,-2,1]) # set inital weights \n",
    "    assert abs(u1.activate(np.array([1,2,3])) - 0.880797) < 1e-5 # verifying that the logistic function calculated accurately\n",
    "    \n",
    "    u1.update(np.array([[1,2,3]]),np.array([0]))#updating the weights with new inputs \n",
    "    assert sum_almost_equal(u1.weights, np.array([2.990752, -2.018496, 0.972257])) #testing to verify our updates were accurate \n",
    "\n",
    "#     u2 = Sigmoid(weights=[0,3,-1])\n",
    "#     u2.update(np.array([[-3,-1,2],[2,1,2]]),np.array([1,0]))\n",
    "#     assert sum_almost_equal(u2.weights, np.array([-0.030739, 2.984961, -1.027437]))\n",
    "#     return True\n",
    "\n",
    "if  test():\n",
    "    print \"All tests completed sucessfully\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
